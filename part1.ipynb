{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cT0zdXOaKpHO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2dZGoRWeLB02"
   },
   "outputs": [],
   "source": [
    "class CNN_Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Model, self).__init__()\n",
    "\n",
    "        # Convolution Layer 1\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Convolution Layer 2\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)  # Output 10 classes (digits 0-9)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dgZ7Y2Q-LHtB",
    "outputId": "66426530-49d3-499e-a5ae-8e776c1f1a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.56MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 135kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 10.5MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_data = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_data = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(test_data, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "aL_GHRF5LKCs"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize the model\n",
    "model = CNN_Model().to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oz7dd9x6LNmZ",
    "outputId": "5f30a3ac-47d3-47d7-a1aa-10094ad6eb27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1572\n",
      "Epoch 2, Loss: 0.0464\n",
      "Epoch 3, Loss: 0.0324\n",
      "Epoch 4, Loss: 0.0248\n",
      "Epoch 5, Loss: 0.0186\n",
      "Epoch 6, Loss: 0.0140\n",
      "Epoch 7, Loss: 0.0114\n",
      "Epoch 8, Loss: 0.0102\n",
      "Epoch 9, Loss: 0.0086\n",
      "Epoch 10, Loss: 0.0050\n",
      "Total Training Time: 175.26s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Total training time\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in trainloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Only print the loss value for each epoch\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")\n",
    "\n",
    "# Calculate total training time after all epochs\n",
    "total_training_time = time.time() - total_start_time\n",
    "print(f\"Total Training Time: {total_training_time:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1c8qCs-uLS1E",
    "outputId": "a29842eb-0f25-40ed-ec97-d148c9b4eae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0664\n",
      "Test Accuracy: 98.25%\n",
      "Test F1 Score: 0.9826\n",
      "Total Training Time: 175.26s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Evaluation on test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculation during evaluation\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Collect predictions and labels for F1 score calculation\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "# Calculate test metrics\n",
    "test_accuracy = 100 * correct / total\n",
    "test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "test_loss = test_loss / len(testloader)\n",
    "\n",
    "# Print each test metric on a separate line\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "g1ANQOseLVcc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "import time  # Importing the time module to measure training time\n",
    "\n",
    "# Hyperparamètres\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Préparer le dataset MNIST\n",
    "transform = Compose([\n",
    "    ToTensor(),\n",
    "    Normalize((0.5,), (0.5,))  # Normalisation pour le MNIST\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "f1g76ePiLYRJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Définition du modèle RCNN pour classification\n",
    "class RCNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(RCNNClassifier, self).__init__()\n",
    "\n",
    "        # Backbone convolutionnel (feature extractor)\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Sortie: 32x28x28\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Sortie: 32x14x14\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Sortie: 64x14x14\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # Sortie: 64x7x7\n",
    "        )\n",
    "\n",
    "        # Region Proposal Network (simplifié pour classification uniquement)\n",
    "        self.rpn = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Sortie: 128x7x7\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # Sortie: 128x3x3\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 3 * 3, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Régularisation\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)  # Extraction de caractéristiques\n",
    "        x = self.rpn(x)          # RPN (simplifié ici pour extraire plus de caractéristiques)\n",
    "        x = self.fc_layers(x)    # Classification finale\n",
    "        return x\n",
    "\n",
    "# Initialisation du modèle\n",
    "model = RCNNClassifier(num_classes=10).to(DEVICE)\n",
    "\n",
    "# Optimiseur et fonction de perte\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Fonction d'entraînement avec calcul du temps\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()\n",
    "    start_time = time.time()  # Enregistrer le début du temps d'entraînement\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            # Zéro du gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Passe avant\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Rétropropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Suivi de la perte\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    end_time = time.time()  # Enregistrer le temps à la fin de l'entraînement\n",
    "    training_time = end_time - start_time  # Calculer le temps total d'entraînement\n",
    "    print(f\"Total training time: {training_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BfTxQvPwMMjE",
    "outputId": "e2578af4-31aa-4a6b-cf1c-7594457030ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.1985\n",
      "Epoch [2/10], Loss: 0.0589\n",
      "Epoch [3/10], Loss: 0.0443\n",
      "Epoch [4/10], Loss: 0.0346\n",
      "Epoch [5/10], Loss: 0.0273\n",
      "Epoch [6/10], Loss: 0.0242\n",
      "Epoch [7/10], Loss: 0.0211\n",
      "Epoch [8/10], Loss: 0.0194\n",
      "Epoch [9/10], Loss: 0.0178\n",
      "Epoch [10/10], Loss: 0.0146\n",
      "Total training time: 176.52 seconds\n",
      "Accuracy: 99.22%\n",
      "Average Loss: 0.0315\n",
      "F1 Score: 0.9922\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score  # Pour calculer la F1 score\n",
    "\n",
    "# Fonction d'évaluation mise à jour\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    total_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            # Passe avant\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Prédictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            # Stocker les étiquettes et prédictions pour F1 score\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    # Calcul des métriques\n",
    "    accuracy = 100 * correct / total\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    f1 = f1_score(all_labels, all_predictions, average='weighted')  # F1 pondérée pour toutes les classes\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Entraîner et évaluer avec affichage complet\n",
    "train_model(model, train_loader, criterion, optimizer, EPOCHS)\n",
    "evaluate_model(model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Performance (Accuracy, F1 Score, Loss) :\n",
    "\n",
    "Le RCNN surpasse le CNN sur toutes les métriques de performance. La complexité ajoutée par l'utilisation des blocs RPN dans le RCNN permet d'extraire des caractéristiques plus discriminatives, ce qui conduit à une meilleure classification.\n",
    "\n",
    "Training Time ::\n",
    "\n",
    "Les deux modèles ont des temps d'entraînement similaires (environ 175-176 secondes), ce qui montre que le RCNN n'ajoute pas une surcharge significative malgré son architecture plus complexe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLM99zUAaFun",
    "outputId": "385cad68-4d62-4445-b97b-5a9a98f00b00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5198, Accuracy: 83.54%, F1 Score: 0.8353, Time: 363.24s\n",
      "Epoch [2/10], Loss: 0.3874, Accuracy: 87.13%, F1 Score: 0.8713, Time: 365.66s\n",
      "Epoch [3/10], Loss: 0.3758, Accuracy: 87.52%, F1 Score: 0.8752, Time: 367.65s\n",
      "Epoch [4/10], Loss: 0.3674, Accuracy: 87.93%, F1 Score: 0.8793, Time: 365.57s\n",
      "Epoch [5/10], Loss: 0.3614, Accuracy: 88.05%, F1 Score: 0.8805, Time: 364.62s\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torchvision import models\n",
    "from sklearn.metrics import f1_score\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "EPOCHS = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Transformations: Resize to 224x224 and convert to RGB (3 channels)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convert grayscale to RGB\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 for VGG16\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalization for MNIST\n",
    "])\n",
    "\n",
    "# Load MNIST dataset with transformations\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Initialize the pretrained VGG16 model and move it to the correct device (GPU or CPU)\n",
    "vgg16 = models.vgg16(pretrained=True).to(DEVICE)\n",
    "\n",
    "# Freeze the pretrained layers of VGG16\n",
    "for param in vgg16.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Modify the classifier to match MNIST (10 classes)\n",
    "vgg16.classifier[6] = nn.Linear(4096, 10).to(DEVICE)  # Ensure the last layer is on the same device\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(vgg16.classifier.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_train_time = 0\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        start_time = time.time()  # Track time at the start of the epoch\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)  # Move data to the same device as the model\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        epoch_time = time.time() - start_time  # Calculate time for this epoch\n",
    "        total_train_time += epoch_time\n",
    "\n",
    "        # Calculate accuracy and F1 score\n",
    "        accuracy = 100 * correct / total\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}, '\n",
    "              f'Accuracy: {accuracy:.2f}%, F1 Score: {f1:.4f}, Time: {epoch_time:.2f}s')\n",
    "\n",
    "    return total_train_time\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)  # Move data to the same device as the model\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}, Test Accuracy: {accuracy:.2f}%, '\n",
    "          f'Test F1 Score: {f1:.4f}')\n",
    "\n",
    "# Train and evaluate the model\n",
    "train_time = train_model(vgg16, train_loader, criterion, optimizer, EPOCHS)\n",
    "evaluate_model(vgg16, test_loader, criterion)\n",
    "\n",
    "print(f'Total Training Time: {train_time:.2f} seconds')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
