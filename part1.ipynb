{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Step 1: Load the Dataset"
      ],
      "metadata": {
        "id": "mJisqO_SX_Jc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import MNIST"
      ],
      "metadata": {
        "id": "FZFkwVUUeccp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN_Model, self).__init__()\n",
        "\n",
        "        # Convolution Layer 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # Convolution Layer 2\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)  # Output 10 classes (digits 0-9)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Wnc7f6y0edjq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "train_data = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "trainloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "testloader = DataLoader(test_data, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "xMnRvfA0egu9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model\n",
        "model = CNN_Model().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "bD8Qzlc8emaj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Total training time\n",
        "total_start_time = time.time()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data in trainloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Only print the loss value for each epoch\n",
        "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(trainloader):.4f}\")\n",
        "\n",
        "# Calculate total training time after all epochs\n",
        "total_training_time = time.time() - total_start_time\n",
        "print(f\"Total Training Time: {total_training_time:.2f}s\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "897ux__JenFr",
        "outputId": "41833330-5a8e-4c21-87e0-e469bd12c565"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.1668\n",
            "Epoch 2, Loss: 0.0473\n",
            "Epoch 3, Loss: 0.0334\n",
            "Epoch 4, Loss: 0.0234\n",
            "Epoch 5, Loss: 0.0190\n",
            "Epoch 6, Loss: 0.0164\n",
            "Epoch 7, Loss: 0.0114\n",
            "Epoch 8, Loss: 0.0105\n",
            "Epoch 9, Loss: 0.0084\n",
            "Epoch 10, Loss: 0.0084\n",
            "Total Training Time: 174.74s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "test_loss = 0.0\n",
        "correct = 0\n",
        "total = 0\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation during evaluation\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Collect predictions and labels for F1 score calculation\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate test metrics\n",
        "test_accuracy = 100 * correct / total\n",
        "test_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "test_loss = test_loss / len(testloader)\n",
        "\n",
        "# Print each test metric on a separate line\n",
        "print(f\"Test Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
        "print(f\"Test F1 Score: {test_f1:.4f}\")\n",
        "print(f\"Total Training Time: {total_training_time:.2f}s\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WPd2Q_3gNQr",
        "outputId": "f1d13880-7eaf-4766-e30e-ab1e1873de73"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0365\n",
            "Test Accuracy: 99.00%\n",
            "Test F1 Score: 0.9900\n",
            "Total Training Time: 174.74s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "# Hyperparamètres\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Préparer le dataset MNIST\n",
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.5,), (0.5,))  # Normalisation pour le MNIST\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Définition du modèle RCNN pour classification\n",
        "class RCNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RCNNClassifier, self).__init__()\n",
        "\n",
        "        # Backbone convolutionnel (feature extractor)\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Sortie: 32x28x28\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Sortie: 32x14x14\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Sortie: 64x14x14\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Sortie: 64x7x7\n",
        "        )\n",
        "\n",
        "        # Region Proposal Network (simplifié pour classification uniquement)\n",
        "        self.rpn = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Sortie: 128x7x7\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)  # Sortie: 128x3x3\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 3 * 3, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),  # Régularisation\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)  # Extraction de caractéristiques\n",
        "        x = self.rpn(x)          # RPN (simplifié ici pour extraire plus de caractéristiques)\n",
        "        x = self.fc_layers(x)    # Classification finale\n",
        "        return x\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = RCNNClassifier(num_classes=10).to(DEVICE)\n",
        "\n",
        "# Optimiseur et fonction de perte\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Fonction d'entraînement\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            # Zéro du gradient\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Passe avant\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Rétropropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Suivi de la perte\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# Fonction d'évaluation\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Entraîner et évaluer\n",
        "train_model(model, train_loader, criterion, optimizer, EPOCHS)\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DwdafjJwsjj1",
        "outputId": "3775ccfa-528f-497d-af25-91c775f93210"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2020\n",
            "Epoch [2/10], Loss: 0.0615\n",
            "Epoch [3/10], Loss: 0.0429\n",
            "Epoch [4/10], Loss: 0.0361\n",
            "Epoch [5/10], Loss: 0.0292\n",
            "Epoch [6/10], Loss: 0.0257\n",
            "Epoch [7/10], Loss: 0.0221\n",
            "Epoch [8/10], Loss: 0.0184\n",
            "Epoch [9/10], Loss: 0.0173\n",
            "Epoch [10/10], Loss: 0.0150\n",
            "Accuracy: 99.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import time  # Importing the time module to measure training time\n",
        "\n",
        "# Hyperparamètres\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 0.001\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Préparer le dataset MNIST\n",
        "transform = Compose([\n",
        "    ToTensor(),\n",
        "    Normalize((0.5,), (0.5,))  # Normalisation pour le MNIST\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Définition du modèle RCNN pour classification\n",
        "class RCNNClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RCNNClassifier, self).__init__()\n",
        "\n",
        "        # Backbone convolutionnel (feature extractor)\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),  # Sortie: 32x28x28\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Sortie: 32x14x14\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),  # Sortie: 64x14x14\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # Sortie: 64x7x7\n",
        "        )\n",
        "\n",
        "        # Region Proposal Network (simplifié pour classification uniquement)\n",
        "        self.rpn = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),  # Sortie: 128x7x7\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)  # Sortie: 128x3x3\n",
        "        )\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 3 * 3, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),  # Régularisation\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)  # Extraction de caractéristiques\n",
        "        x = self.rpn(x)          # RPN (simplifié ici pour extraire plus de caractéristiques)\n",
        "        x = self.fc_layers(x)    # Classification finale\n",
        "        return x\n",
        "\n",
        "# Initialisation du modèle\n",
        "model = RCNNClassifier(num_classes=10).to(DEVICE)\n",
        "\n",
        "# Optimiseur et fonction de perte\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Fonction d'entraînement avec calcul du temps\n",
        "def train_model(model, train_loader, criterion, optimizer, epochs):\n",
        "    model.train()\n",
        "    start_time = time.time()  # Enregistrer le début du temps d'entraînement\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            # Zéro du gradient\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Passe avant\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Rétropropagation\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Suivi de la perte\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    end_time = time.time()  # Enregistrer le temps à la fin de l'entraînement\n",
        "    training_time = end_time - start_time  # Calculer le temps total d'entraînement\n",
        "    print(f\"Total training time: {training_time:.2f} seconds\")\n",
        "\n",
        "# Fonction d'évaluation\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Entraîner et évaluer\n",
        "train_model(model, train_loader, criterion, optimizer, EPOCHS)\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tczITko4tjXW",
        "outputId": "d0109887-9369-46fe-d662-cee9951d1866"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.2092\n",
            "Epoch [2/10], Loss: 0.0609\n",
            "Epoch [3/10], Loss: 0.0464\n",
            "Epoch [4/10], Loss: 0.0352\n",
            "Epoch [5/10], Loss: 0.0279\n",
            "Epoch [6/10], Loss: 0.0264\n",
            "Epoch [7/10], Loss: 0.0207\n",
            "Epoch [8/10], Loss: 0.0199\n",
            "Epoch [9/10], Loss: 0.0172\n",
            "Epoch [10/10], Loss: 0.0139\n",
            "Total training time: 173.45 seconds\n",
            "Accuracy: 99.27%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D7nMrtrZvfHm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}