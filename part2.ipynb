{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jOKOisBvDz1m"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zEM4z9Q7EDXk"
   },
   "outputs": [],
   "source": [
    "# Définir les transformations pour le dataset MNIST\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  # Convertir en 3 canaux\n",
    "    transforms.Resize((128, 128)),  # Redimensionner les images à 128x128 pixels\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Charger les ensembles d'entraînement et de test\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "t5lAyY20EH3j"
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=128, patch_size=32, in_chans=3, embed_dim=512):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_chans = in_chans\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.num_patches = (img_size // patch_size) ** 2\n",
    "        self.patch_dim = in_chans * patch_size * patch_size\n",
    "        self.proj = nn.Linear(self.patch_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = x.reshape(B, C, H // self.patch_size, self.patch_size, W // self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 4, 3, 5, 1).reshape(B, (H // self.patch_size) * (W // self.patch_size), -1)\n",
    "        x = self.proj(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t5JIOPp1EMIX"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, img_size=128, patch_size=32, in_chans=3, num_classes=10, embed_dim=512, depth=4, num_heads=4, mlp_dim=1024, dropout=0.1):\n",
    "        super(ViT, self).__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, 1 + self.patch_embed.num_patches, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=mlp_dim, dropout=dropout, activation='gelu', batch_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x[:, 0])\n",
    "        x = self.head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDaYBugSETIK",
    "outputId": "bf560cac-79b0-4ca9-a1e7-75ccbce970d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-791a035daba5>:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# Initialiser le modèle ViT\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_vit = ViT(img_size=128, patch_size=32, in_chans=3, num_classes=10, embed_dim=512, depth=4, num_heads=4, mlp_dim=1024, dropout=0.1)\n",
    "model_vit = model_vit.to(device)\n",
    "\n",
    "# Définir la fonction de perte et l'optimiseur avec un taux d'apprentissage réduit\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_vit = optim.Adam(model_vit.parameters(), lr=0.00001)\n",
    "\n",
    "# Utiliser ReduceLROnPlateau pour ajuster le taux d'apprentissage\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_vit, 'min', patience=2, factor=0.5)\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P2HpkeACEaZy",
    "outputId": "4d20af8a-8c0a-4a70-d93f-c381854f4645"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-6a7792d87ebd>:12: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.7009\n",
      "Epoch [2/10], Loss: 0.3282\n",
      "Epoch [3/10], Loss: 0.2419\n",
      "Epoch [4/10], Loss: 0.1968\n",
      "Epoch [5/10], Loss: 0.1664\n",
      "Epoch [6/10], Loss: 0.1447\n",
      "Epoch [7/10], Loss: 0.1259\n",
      "Epoch [8/10], Loss: 0.1137\n",
      "Epoch [9/10], Loss: 0.1012\n",
      "Epoch [10/10], Loss: 0.0912\n",
      "Training Time: 1718.21 seconds\n"
     ]
    }
   ],
   "source": [
    "import time  # Importer le module time pour mesurer la durée\n",
    "\n",
    "# Fonction d'entraînement avec Mixed Precision\n",
    "def train_vit_mixed_precision(model, train_loader, optimizer, criterion, scaler, scheduler, num_epochs=10):\n",
    "    model.train()\n",
    "    start_time = time.time()  # Début du chronomètre\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item()\n",
    "        scheduler.step(running_loss / len(train_loader))  # Mettre à jour le scheduler\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "    total_time = time.time() - start_time  # Calcul du temps écoulé\n",
    "    print(f'Training Time: {total_time:.2f} seconds')  # Afficher le temps total\n",
    "\n",
    "# Entraîner le modèle ViT avec Mixed Precision\n",
    "train_vit_mixed_precision(model_vit, train_loader, optimizer_vit, criterion, scaler, scheduler, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDFq1isfD693",
    "outputId": "9fb228a6-2a59-4afa-e2ad-99eccf5859bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ViT Accuracy: 96.84%\n",
      "ViT F1 Score: 0.9683\n",
      "ViT Average Loss: 0.1100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Fonction d'évaluation\n",
    "def evaluate_vit(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    return accuracy, f1, avg_loss\n",
    "\n",
    "# Évaluer le modèle ViT\n",
    "vit_accuracy, vit_f1, vit_loss = evaluate_vit(model_vit, test_loader, criterion)\n",
    "print(f\"ViT Accuracy: {vit_accuracy:.2f}%\")\n",
    "print(f\"ViT F1 Score: {vit_f1:.4f}\")\n",
    "print(f\"ViT Average Loss: {vit_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interprétation des Résultats Obtenus et Comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Metric\tViT\tRCNN\tCNN\n",
    "Accuracy\t96.84%\t99.22%\t98.25%\n",
    "F1 Score\t0.9683\t0.9922\t0.9826\n",
    "Average Loss\t0.1100\t0.0315\t0.0664\n",
    "Training Time\t1718.21 seconds\t176.52 seconds\t175.26 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance (Précision et F1 Score) :\n",
    "\n",
    "Le modèle ViT atteint une précision de 96.84% et un F1 score de 0.9683, ce qui est inférieur aux performances de RCNN et CNN.\n",
    "Le RCNN obtient les meilleures performances avec une précision de 99.22% et un F1 score de 0.9922, suivi par le CNN avec 98.25% de précision et un F1 score de 0.9826.\n",
    "La performance de ViT est donc moins bonne que celle des modèles CNN et RCNN, probablement en raison de la simplicité du dataset MNIST. Les modèles basés sur les convolutions sont mieux adaptés à ce genre de tâche simple, tandis que les Vision Transformers nécessitent généralement plus de données et des tâches plus complexes pour révéler leur plein potentiel.\n",
    "Perte Moyenne :\n",
    "\n",
    "La perte moyenne de ViT est 0.1100, ce qui est plus élevé que celle de RCNN (0.0315) et CNN (0.0664). Cela montre que ViT a eu plus de difficultés à minimiser la fonction de perte pendant l'entraînement, ce qui est cohérent avec sa performance moins bonne.\n",
    "Temps d'Entraînement :\n",
    "\n",
    "Le temps d'entraînement pour ViT est beaucoup plus long (1718.21 secondes) par rapport à RCNN et CNN, qui ont pris environ 176 secondes chacune.\n",
    "Ce temps d'entraînement plus long est dû à la complexité du modèle ViT, qui utilise des mécanismes d'attention globale (self-attention) plus coûteux en termes de calcul, en particulier lorsqu'il est formé à partir de zéro. De plus, ViT nécessite souvent des jeux de données plus vastes et des préformations pour être efficace, ce qui n'est pas le cas sur MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
